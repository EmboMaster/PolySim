#!/usr/bin/env python3
"""
ManiSkillè®­ç»ƒç¤ºä¾‹è„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ManiSkillä»¿çœŸå™¨è¿›è¡Œæœºå™¨äººæ“ä½œä»»åŠ¡è®­ç»ƒ
"""

import sys
import os
import torch
import numpy as np
from pathlib import Path
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from humanoidverse.simulator.maniskill.maniskill import ManiSkill
from humanoidverse.simulator.maniskill.maniskill_cfg import ManiSkillCfg
from omegaconf import OmegaConf


def create_maniskill_config(task_name="PickCube-v1", robot_type="panda", num_envs=4):
    """åˆ›å»ºManiSkillé…ç½®"""
    
    config = OmegaConf.create({
        'simulator': {
            '_target_': 'humanoidverse.simulator.maniskill.maniskill.ManiSkill',
            'config': {
                'name': 'maniskill',
                'sim': {
                    'fps': 200,
                    'control_decimation': 4,
                    'substeps': 1,
                    'gravity': [0.0, 0.0, -9.81],
                    'solver_iterations': 4,
                    'solver_velocity_iterations': 1,
                    'render_mode': 'human',
                    'render_interval': 4
                },
                'scene': {
                    'num_envs': num_envs,
                    'env_spacing': 4.0,
                    'replicate_physics': True
                },
                'task': {
                    'task_name': task_name,
                    'control_mode': 'pd_ee_delta_pose',
                    'obs_mode': 'state',
                    'reward_mode': 'dense'
                },
                'robot': {
                    'robot_type': robot_type,
                    'asset_path': None
                },
                'env': {
                    'max_episode_steps': 1000,
                    'action_scale': 1.0,
                    'reward_scale': 1.0
                }
            }
        },
        'robot': {
            'asset': {
                'robot_type': robot_type,
                'asset_path': None
            }
        },
        'terrain': {
            'mesh_type': 'plane'
        },
        'num_envs': num_envs,
        'headless': False,
        'domain_rand': {
            'randomize_friction': True,
            'friction_range': [0.5, 1.5],
            'randomize_mass': True,
            'mass_range': [0.8, 1.2],
            'randomize_com': True,
            'com_range': [-0.1, 0.1]
        }
    })
    
    return config


def train_maniskill_agent(config, num_episodes=1000):
    """è®­ç»ƒManiSkillæ™ºèƒ½ä½“"""
    
    print("å¼€å§‹ManiSkillæ™ºèƒ½ä½“è®­ç»ƒ...")
    
    # åˆ›å»ºä»¿çœŸå™¨
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    simulator = ManiSkill(config, device)
    simulator.set_headless(False)
    
    try:
        # è®¾ç½®ä»¿çœŸå™¨
        simulator.setup()
        print("âœ“ ä»¿çœŸå™¨è®¾ç½®å®Œæˆ")
        
        # è®¾ç½®åœ°å½¢
        simulator.setup_terrain('plane')
        print("âœ“ åœ°å½¢è®¾ç½®å®Œæˆ")
        
        # åŠ è½½æœºå™¨äººèµ„äº§
        dof_info = simulator.load_assets(config.robot)
        if dof_info:
            num_dof, num_bodies, dof_names, body_names = dof_info
            print(f"âœ“ æœºå™¨äººåŠ è½½æˆåŠŸ: {num_dof} DOFs, {num_bodies} bodies")
        else:
            print("âš  æœºå™¨äººåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            simulator.num_dof = 7  # Pandaæœºå™¨äººé»˜è®¤7ä¸ªå…³èŠ‚
            simulator.num_bodies = 8  # åŒ…æ‹¬åŸºåº§
        
        # åˆ›å»ºç¯å¢ƒ
        num_envs = config.num_envs
        env_origins = torch.zeros(num_envs, 3)
        base_init_state = torch.zeros(7 + simulator.num_dof)  # ä½ç½®(3) + å››å…ƒæ•°(4) + å…³èŠ‚ä½ç½®
        
        envs, handles = simulator.create_envs(
            num_envs, env_origins, base_init_state, config
        )
        print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {len(envs)} ä¸ªç¯å¢ƒ")
        
        # å‡†å¤‡ä»¿çœŸ
        simulator.prepare_sim()
        print("âœ“ ä»¿çœŸå‡†å¤‡å®Œæˆ")
        
        # è®¾ç½®æŸ¥çœ‹å™¨
        if not config.headless:
            simulator.setup_viewer()
            print("âœ“ æŸ¥çœ‹å™¨è®¾ç½®å®Œæˆ")
        
        # è®­ç»ƒå¾ªç¯
        print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {num_episodes} ä¸ªepisode...")
        
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(num_episodes):
            # é‡ç½®ç¯å¢ƒ
            simulator.reset_envs(torch.arange(num_envs, device=device))
            
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done and episode_length < 1000:
                # éšæœºåŠ¨ä½œï¼ˆè¿™é‡Œå¯ä»¥æ›¿æ¢ä¸ºå®é™…çš„ç­–ç•¥ç½‘ç»œï¼‰
                actions = torch.randn(num_envs, simulator.num_dof) * 0.1
                
                # æ‰§è¡Œä»¿çœŸæ­¥éª¤
                simulator.step(actions)
                
                # è·å–è§‚å¯Ÿå’Œå¥–åŠ±
                observations = simulator.get_observations()
                
                # è®¡ç®—å¥–åŠ±ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å®ç°ï¼‰
                rewards = torch.ones(num_envs, device=device) * 0.1
                episode_reward += rewards.mean().item()
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å®ç°ç»ˆæ­¢æ¡ä»¶
                if episode_length > 500:  # ç®€å•çš„ç»ˆæ­¢æ¡ä»¶
                    done = True
                
                episode_length += 1
                
                # æ¸²æŸ“
                if not config.headless:
                    simulator.render()
            
            # è®°å½•episodeä¿¡æ¯
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # æ‰“å°è¿›åº¦
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(episode_rewards[-10:])
                avg_length = np.mean(episode_lengths[-10:])
                print(f"Episode {episode + 1}/{num_episodes} - "
                      f"Avg Reward: {avg_reward:.3f}, "
                      f"Avg Length: {avg_length:.1f}")
        
        # è®­ç»ƒå®Œæˆ
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f}")
        print(f"æœ€ç»ˆå¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f}")
        
        return episode_rewards, episode_lengths
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None
        
    finally:
        # å…³é—­ä»¿çœŸå™¨
        simulator.close()
        print("âœ“ ä»¿çœŸå™¨å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ManiSkillè®­ç»ƒç¤ºä¾‹")
    parser.add_argument("--task", type=str, default="PickCube-v1", 
                       help="ä»»åŠ¡åç§°")
    parser.add_argument("--robot", type=str, default="panda", 
                       help="æœºå™¨äººç±»å‹")
    parser.add_argument("--num_envs", type=int, default=4, 
                       help="ç¯å¢ƒæ•°é‡")
    parser.add_argument("--num_episodes", type=int, default=100, 
                       help="è®­ç»ƒepisodeæ•°é‡")
    parser.add_argument("--headless", action="store_true", 
                       help="æ— å¤´æ¨¡å¼")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ManiSkillè®­ç»ƒç¤ºä¾‹")
    print("=" * 60)
    print(f"ä»»åŠ¡: {args.task}")
    print(f"æœºå™¨äºº: {args.robot}")
    print(f"ç¯å¢ƒæ•°é‡: {args.num_envs}")
    print(f"è®­ç»ƒepisode: {args.num_episodes}")
    print(f"æ— å¤´æ¨¡å¼: {args.headless}")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = create_maniskill_config(
        task_name=args.task,
        robot_type=args.robot,
        num_envs=args.num_envs
    )
    config.headless = args.headless
    
    # å¼€å§‹è®­ç»ƒ
    rewards, lengths = train_maniskill_agent(config, args.num_episodes)
    
    if rewards is not None:
        print("\nè®­ç»ƒç»“æœ:")
        print(f"æ€»episode: {len(rewards)}")
        print(f"å¹³å‡å¥–åŠ±: {np.mean(rewards):.3f} Â± {np.std(rewards):.3f}")
        print(f"å¹³å‡é•¿åº¦: {np.mean(lengths):.1f} Â± {np.std(lengths):.1f}")
        print(f"æœ€ä½³å¥–åŠ±: {np.max(rewards):.3f}")
        print(f"æœ€å·®å¥–åŠ±: {np.min(rewards):.3f}")


if __name__ == "__main__":
    main()
